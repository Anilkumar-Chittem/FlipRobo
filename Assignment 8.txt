Machine learning
1.B
2. A
3. C
4. C
5. D
6. C
7. c
8. A, D
9. B, D
10. A, B, D
11. One hot encoding is not advisable in case the variable to be encoded has very high or very low
number of categories. In case of high number of categories (say 15), one hot encoding will create
14 new features if we drop the first feature, which will result in low feature importance of each of
these 14 features. And in case of low number of categories (say 3 or 4), one hot encoding will
create 2 to 3 number of features when drop first is true, this will result in highly correlated features
and while building the model some of these features will be eliminated even if they are important.
To avoid these problems one can use frequency encoding or mean encoding. In frequency
encoding each of the category is coded based on its frequency and in mean it is encoded as per
mean.
12. In case of data imbalance, we can either under sample the major class or over sample the minor
class. under sampling is often unadvisable because while under sampling the problem of data loss
occurs, which further results in less generic models. Also, data is quite expensive and must not be
wasted. Following are some methods for dealing with data imbalance.
RANDOM UNDERSAMPLER: In random undersampler, the data points of major class are dropped
randomly so that the data points of major class in sample becomes equal to data points of minor
class. This method has a problem of data loss associated with it as most of the data is dropped
while fitting it to dataset. Data is expensive and it is the key to a data science algorithm. Therefore, it
is not preferred in actual practice.
RANDOM OVERSAMPLER: Random Oversampler makes random replications of data points of
minor class by multiplying them to a random weight so as to make the number of data points of
minor class become equal to data points of major class. It is less preferred over SMOTE and
ADASYN because, it merely makes duplication of data and hence the model built has no added
value.
SMOTE (Synthetic Minority Oversampling Technique): Synthetic Minority Oversampling
Technique works by selecting examples that are close in the feature space, drawing a line between
the examples in the feature space and drawing a new sample at a point along that line. Specifically,
a random example from the minority class is first chosen. Then k of the nearest neighbors for that
example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example
is created at a randomly selected point between the two examples in feature space.
ADASYN (Adaptive Synthetic): ADASYN also generates synthetic samples as done by SMOTE.
But it takes care of the distribution of the data points. It generates synthetic samples where the
crowding of the minority samples is comparatively less. In this case the final dataset has similar
distribution of all the samples in the feature space.
13. Same as answer 12


14. Grid search CV stands for Grid Search Cross Validation. It is used for hyper parameter tuning the
machine learning models. In grid search cv we make a params grid which is a dictionary containing
different values of each parameter we want to tune. We also provide number of folds of cross
validation and the ML model on which the grid search is being performed. Grid search fits all the
possible combinations of each and every parameter mentioned in params grid with number of folds
of cross validation. This gives the best possible score and hence the best parameters for the model
to be fit. In this way we get the best performing set and we build our final model on these parameters.
Gridsearch CV is disadvantageous over large datasets as it does a large number of fits which takes
a lot of time and is computationally very heavy. But it is very helpful tool if the dataset is not very large.
15. The evaluation metrics used to evaluate a regression model are as follows:
R2 Score: Also known as coefficient of determination. R2 signifies that what fraction of total variance
is explained by the best fit line. It is calculated by the following formula:
2


Adjusted R2 Score: Adjusted R2 and R2 both represent that how well the model fits the data points.
But adjusted R2 penalizes the model for using more features. In case we increase the number of
features in training data the R2 will increase but adjusted R2 will only increase if the new feature
adds value to our model. Due to this reason adjusted R2

is considered as a better evaluation metric

than R2
. Adjusted R2

is always less than or equal to R2

. The formula to calculate adjusted R2
is as

follows:

Where, n = number of data points in the dataset

K = Number of features in the dataset excluding the constant term

Mean Squared Error (MSE): Mean Squared error is the mean of all the squares of the distances of
each data point from its predictions by the best fit line. It is always positive and is calculated as follows:


Root Mean Squared Error (RMSE): RMSE is the standard deviation of the residuals (prediction
errors). Residuals are a measure of how far from the regression line data points are; RMSE is a
measure of how spread out these residuals are. In other words, it tells you how concentrated the data
is around the line of best fit. It is nothing but square root of MSE.

Statistics :
1. a. The probability of rejecting H0 when H1 is true
2. b. null hypothesis
3. d. Type I error
4. b. the t distribution with n - 1 degrees of freedom
5. a. accepting H0 when it is false
6. d. a two-tailed test
7. b. the probability of committing a Type I error
8. a. the probability of committing a Type II error
9. a. z > zÎ±
10. c. the level of significance
11. a. level of significance
12. d. All of the Above
13. The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically
significant differences between the means of two or more independent (unrelated) groups (although you tend
to only see it used when there are a minimum of three, rather than two groups).

14. There are three primary assumptions in ANOVA: The responses for each factor level have a normal
population distribution. These distributions have the same variance. The data are independent.

15. A one-way ANOVA only involves one factor or independent variable, whereas there are two independent
variables in a two-way ANOVA. In a one-way ANOVA, the one factor or independent variable analysed has
three or more categorical groups. A two-way ANOVA instead compares multiple groups of two factors


Python:
1.C
2.B
3.C
4.A
5.D
6.C
7.A
8.C
9.A, C
10.A, B
11.# Program to find the factorial of a no. entered by user

n=int(input("enter the number to find the factorial"))
def fact(n):
    
    if n==1 :
        return 1
    else :
        return n*fact(n-1)
    
if n<0 :
    print(" number is negative . factorial not possible")
elif n==0:
    print("The factorial is ",1)
else:
     print("The factorial is ",fact(n))
12.# composite or prime
number=int(input("Enter an number"))
if number >1:
    for i in range(2,number):
       
        if (number%i==0):
            print("composite number")
            break
        else:
            print("prime number")
            
13.# palindrome or not
pd=str(input(" Enter a string  "))

   
if pd==pd[::-1]:
        print("palindrome")
else:
        print("not palindorome")
        
14.# Program to find the length of hypotenuse of a right angled triangle

import math
a=int(input("enter the base of the right angled triangle"))
b=int(input("enter the perpendicular of the right angled triangle"))
print("the length of hypotenuse is ",math.sqrt(a**2+b**2))
15.# Program to find the frequency of each character present in the entered string

string=input("Enter the string")
freq=dict()
for i in string :
    if i in freq.keys():
        freq[i]=freq[i]+1
    else :
        freq[i]=1
print(" The frequencies of characters in the string are", freq)