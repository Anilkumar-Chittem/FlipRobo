Statistics:

1.d
2.c
3.c
4.b
5.c
6.b
7.a
8.a
9.d
10.a

Machine learning:

1.R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model
A residual sum of squares (RSS) measures the level of variance in the error term, or residuals, of a regression mode.
R2 square doesnt get affect by scale where as rss gets affected so it is best to use r2 square than rss.

2.The residual sum of squares is used to help you decide if a statistical model is a good fit for your data. 
It measures the overall difference between your data and the values predicted by your estimation model
 Total SS = Explained SS + Residual Sum of Squares The Total SS (TSS or SST) tells you how much variation there is in the dependent variable.
The Explained SS tells you how much of the variation in the dependent variable your model explained.
Explained SS = Σ(Y-Hat – mean of Y)2
Total SS = Σ(Yi – mean of Y)2.

3.This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. 
  In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting
The fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function.
Now, this will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data.
 This is where regularization comes in and shrinks or regularizes these learned estimates towards zero

4.Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. 
But what is actually meant by ‘impurity’, If all the elements belong to a single class then it can be called pure.
lies b/w 0 to 1 

5.Decision trees are prone to overfitting, especially when a tree is particularly deep.
 This is due to the amount of specificity we look at leading to smaller sample of events that meet the previous assumptions. 

6.Ensemble methods are techniques that create multiple models and then combine them to produce improved results.
 Ensemble methods usually produces more accurate solutions than a single model would.

7.Bagging is a method of merging the same type of predictions. Boosting is a method of merging different types of predictions.
Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance.
In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance.
Models are built independently in Bagging. New models are affected by a previously built model’s performance in Boosting

8.The out-of-bag (OOB) error is the average error for each  calculated z using predictions from the trees that do not contain  in their  z respective bootstrap sample
z=(x,y)

9.Cross-validation is a statistical method used to estimate the skill of machine learning models.
That k-fold cross validation is a procedure used to estimate the skill of the model on new data.
The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.
 As such, the procedure is often called k-fold cross-validation

10., hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.
 A hyperparameter is a parameter whose value is used to control the learning process

11.When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error. […] When the learning rate is too small, training is not only slower, but may become permanently stuck with a high training error.

12.no it can not be used for no linear data it can be used only for binomial classification only 

13.AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem.
 This makes Gradient Boosting more flexible than AdaBoost

14.Bias is the simplifying assumptions made by the model to make the target function easier to approximate.

15.radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms.
Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line
the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models

Sql.




